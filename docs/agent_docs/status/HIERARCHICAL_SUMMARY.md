# Conversation Memory V2: 100만 토큰 처리를 위한 계층적 요약 전략

**작성자**: Manus AI
**날짜**: 2026년 1월 9일

## 1. 문제 정의: 100만 토큰의 도전

100만 토큰은 약 500페이지 분량의 방대한 텍스트입니다. 일반적인 AI 모델의 컨텍스트 윈도우(대부분 200K 토큰 미만)를 훨씬 초과하며, 이를 단일 프로세스로 처리하는 것은 다음과 같은 심각한 문제를 야기합니다.

| 문제점 | 설명 |
|---|---|
| **컨텍스트 유실** | 모델의 컨텍스트 윈도우를 초과하는 순간, 가장 오래된 정보부터 순차적으로 삭제됩니다. 이는 장기 프로젝트의 핵심적인 초기 결정사항이나 맥락을 잃게 만듭니다. |
| **성능 병목** | 단일 프로세스로 100만 토큰을 순차적으로 요약하면 수십 분에서 몇 시간이 걸릴 수 있습니다. 이는 실시간 상호작용을 불가능하게 합니다. |
| **비용 증가** | 전체 텍스트를 반복적으로 컨텍스트에 포함시켜 요약하면, 중복되는 계산으로 인해 API 호출 비용이 기하급수적으로 증가합니다. |
| **메모리 한계** | 100만 토큰을 메모리에 모두 로드하는 것은 대부분의 단일 Node.js 프로세스의 힙 메모리(약 4GB) 한계를 초과할 수 있습니다. |

Conversation Memory V2는 이러한 문제를 해결하기 위해 **계층적 요약(Hierarchical Summarization)** 전략을 채택했습니다.

## 2. 계층적 요약 아키텍처

계층적 요약은 '분할 정복(Divide and Conquer)' 철학에 기반합니다. 거대한 텍스트를 작은 조각(Chunk)으로 나눈 뒤, 각 조각을 독립적으로 요약하고, 그 요약본들을 다시 그룹화하여 상위 레벨의 요약을 만드는 과정을 반복합니다.

### 2.1. 아키텍처 다이어그램

```mermaid
graph TD
    subgraph Level 0: 원본 대화 (1,000,000 토큰)
        C1["Chunk 1 (500)"]
        C2["Chunk 2 (500)"]
        C3["..."]
        C2000["Chunk 2000 (500)"]
    end

    subgraph "병렬 요약 (Parallel Workers x20)"
        W1[Worker 1]
        W2[...]
        W20[Worker 20]
    end

    subgraph Level 1: 청크 요약 (300,000 토큰)
        S1["Summary 1 (150)"]
        S2["Summary 2 (150)"]
        S3["..."]
        S2000["Summary 2000 (150)"]
    end

    subgraph Level 2: 1차 병합 (200,000 토큰)
        M1["Merged 1 (500)"]
        M2["..."]
        M400["Merged 400 (500)"]
    end

    subgraph Level 3: 2차 병합 (64,000 토큰)
        H1["Hierarchical 1 (800)"]
        H2["..."]
        H80["Hierarchical 80 (800)"]
    end

    subgraph Level 4: 최종 압축 컨텍스트 (19,200 토큰)
        F1["Final Context 1 (1200)"]
        F2["..."]
        F16["Final Context 16 (1200)"]
    end

    C1 --> W1
    C2 --> W1
    C3 --> W2
    C2000 --> W20

    W1 --> S1
    W1 --> S2
    W2 --> S3
    W20 --> S2000

    S1 -- 5개씩 그룹화 --> M1
    S2 -- 5개씩 그룹화 --> M1
    S3 -- 5개씩 그룹화 --> M1

    M1 -- 5개씩 그룹화 --> H1
    M2 -- 5개씩 그룹화 --> H1

    H1 -- 5개씩 그룹화 --> F1
    H2 -- 5개씩 그룹화 --> F1
```

### 2.2. 단계별 상세 설명

#### **Level 0: 청킹 (Chunking)**
- **입력**: 1,000,000 토큰의 전체 대화 기록
- **프로세스**: 대화를 500토큰 단위의 작은 **청크(Chunk)** 2,000개로 분할합니다. 각 청크는 처리 가능한 독립적인 단위가 됩니다.
- **출력**: 2,000개의 청크 (각 500 토큰)

#### **Level 1: 병렬 요약 (Parallel Summarization)**
- **입력**: 2,000개의 청크
- **프로세스**: 20개의 **병렬 워커(Parallel Worker)**가 청크들을 나눠 동시에 요약합니다. 각 워커는 100개의 청크를 담당하며, 각 청크를 150토큰의 구조화된 JSON 요약(핵심 결정, 코드, 태그 포함)으로 압축합니다.
- **출력**: 2,000개의 청크 요약 (각 150 토큰, 총 300,000 토큰)
- **효과**: 2,000개의 순차 처리에 4,000초(약 67분)가 걸릴 작업을 **200초(약 3.3분)** 만에 완료합니다.

#### **Level 2: 1차 병합 (First Merge)**
- **입력**: 2,000개의 청크 요약
- **프로세스**: 청크 요약 5개를 하나의 그룹으로 묶어, LLM을 사용하지 않는 **기계적 병합(Mechanical Merge)**을 통해 중복 정보를 제거하고 하나의 상위 컨텍스트로 통합합니다. 이 과정에서 400개의 병합 컨텍스트가 생성됩니다.
- **출력**: 400개의 병합 컨텍스트 (각 500 토큰, 총 200,000 토큰)

#### **Level 3: 2차 병합 (Second Merge)**
- **입력**: 400개의 병합 컨텍스트
- **프로세스**: Level 2와 동일한 방식으로, 1차 병합된 컨텍스트 5개를 다시 그룹화하여 80개의 더 높은 수준의 계층적 컨텍스트로 재병합합니다.
- **출력**: 80개의 계층적 컨텍스트 (각 800 토큰, 총 64,000 토큰)

#### **Level 4: 최종 병합 (Final Merge)**
- **입력**: 80개의 계층적 컨텍스트
- **프로세스**: 마지막으로, 80개의 컨텍스트를 다시 5개씩 그룹화하여 16개의 최종 압축 컨텍스트를 생성합니다. 이 16개의 컨텍스트가 100만 토큰 대화의 정수를 담고 있는 최종 결과물입니다.
- **출력**: 16개의 최종 압축 컨텍스트 (각 1,200 토큰, 총 19,200 토큰)

## 3. 결과 및 효과

| 항목 | 단일 순차 요약 (기존 방식) | 계층적 요약 (V2 방식) |
|---|---|---|
| **최종 토큰 수** | ~200,000 토큰 (컨텍스트 초과로 유실) | **~19,200 토큰** |
| **압축률** | ~80% (정보 유실) | **~98% (정보 보존)** |
| **처리 시간** | ~67분 | **~3.3분** |
| **비용** | 높음 (중복 계산) | **최적화** (독립적 병렬 처리) |
| **정보 보존** | 낮음 (오래된 정보 삭제) | **높음** (모든 청크의 핵심 정보 유지) |

## 4. 결론

Conversation Memory V2의 계층적 요약 전략은 단순한 압축을 넘어, **대규모 컨텍스트를 실시간으로 처리하고, 비용을 최적화하며, 가장 중요한 정보의 무결성을 보장하는** 핵심적인 아키텍처입니다.

이 방식을 통해 100만 토큰이라는 방대한 대화 기록도 단 몇 분 만에 2만 토큰 미만의 관리 가능한 컨텍스트로 변환할 수 있으며, 이는 장기 프로젝트의 성공적인 수행을 위한 필수적인 기술입니다.
